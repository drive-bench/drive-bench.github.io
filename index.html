<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives">
  <meta name="keywords" content="Autonomous Driving, Vision-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DriveBench - Project Page</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Nunito" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <!-- <script src="./static/js/index.js"></script> -->
</head>


<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <!-- <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div> -->
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/ldkong1205/Robo3D" target="_blank">
            ICCV 2023 | Robo3D 
          </a>
          <a class="navbar-item" href="https://github.com/ldkong1205/RoboDepth" target="_blank">
            NeurIPS 2023 | RoboDepth
          </a>
          <a class="navbar-item" href="https://robodrive-24.github.io/" target="_blank">
            ICRA 2024 | RoboDrive
          </a>
          <a class="navbar-item" href="https://github.com/Daniel-xsy/RoboBEV" target="_blank">
            TPAMI 2024 | RoboBEV
          </a>
          <a class="navbar-item" href="https://github.com/OpenDriveLab/DriveLM" target="_blank">
            ECCV 2024 | DriveLM
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title">
            Are VLMs Ready for Autonomous Driving?<br/>An Empirical Study from the Reliability, Data, and Metric Perspectives
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://daniel-xsy.github.io/" target="_blank">
                <b>Shaoyuan Xie</b></a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ldkong.com/" target="_blank">
                <b>Lingdong Kong</b></a><sup>2,3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=kMui170AAAAJJ&hl=en" target="_blank">
                <b>Yuhao Dong</b></a><sup>2,4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=dgYJ6esAAAAJJ&hl=en" target="_blank">
                <b>Chonghao Sima</b></a><sup>2,5</sup>
            </span>
            <br/>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=QDXADSEAAAAJJ&hl=en" target="_blank">
                <b>Wenwei Zhang</b></a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ics.uci.edu/~alfchen/" target="_blank">
                <b>Qi Alfred Chen</b></a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://liuziwei7.github.io/" target="_blank">
                <b>Ziwei Liu</b></a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ&hl=en" target="_blank">
                <b>Liang Pan</b></a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>University of California, Irvine&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>2</sup>Shanghai AI Laboratory&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>3</sup>National University of Singapore
            </span>
            <span class="author-block">
              <sup>4</sup>S-Lab, Nanyang Technological University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>5</sup>The University of Hong Kong
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors" style="font-size: 0.5rem;">
            <span class="author-block">
              <sup>&#8225</sup>Project Lead&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>&#167;</sup>Corresponding Author
            </span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(66, 133, 244); color: white; border-color: rgb(66, 133, 244);">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span><b>Paper</b></span>
                </a>
              </span>

              &nbsp;
              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(192, 0, 0); color: white; border-color: rgb(192, 0, 0);">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span><b>arXiv</b></span>
                </a>
              </span>
              &nbsp;
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/drive-bench/toolkit" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(66, 133, 244); color: white; border-color: rgb(66, 133, 244);">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span><b>Toolkit</b></span>
                  </a>
              </span>
              &nbsp;
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/drive-bench/arena" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(192, 0, 0); color: white; border-color: rgb(192, 0, 0);">
                  <span class="icon"><i class="fa fa-database" aria-hidden="true"></i></span>
                  <span><b>Dataset</b></span>
                  </a>
              </span>
              &nbsp;
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(66, 133, 244); color: white; border-color: rgb(66, 133, 244);">
                  <span class="icon"><i class="fa fa-balance-scale" aria-hidden="true"></i></span>
                  <span><b>Leaderboard</b></span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The rapid advancements in Vision-Language Models (VLMs) have sparked significant 
            interest in their application to autonomous driving, with a focus on achieving 
            interpretable driving decisions through natural language interactions. In this work, 
            we question the fundamental assumption that VLMs inherently provide visual-grounded 
            explanations in driving contexts thus improving driving interpretability. 
            The assumption has been widely acknowledged yet systematically examined.
            Through extensive experiments across 17 conditions (including clean image, 
            corrupted image, and text-only inputs), 12 models, and 5 task domains, 
            we comprehensively investigate the reliability of VLMs' visual-grounded 
            explanations in driving scenarios. Our findings reveal that VLMs frequently 
            fabricate plausible driving responses from general knowledge or subtle text cues, 
            especially in scenarios with degraded or absent visual information.
            However, the phenomenon is long-hiding due to significant problems 
            in terms of dataset imbalances, and unreliable evaluations within 
            current driving-with-language benchmarks, which fail to effectively assess 
            the contextual and safety-oriented requirements of autonomous driving tasks. 
            Instead of pushing for the state-of-the-art model under problematical setups, 
            our works highlight the need to rethink the current dataset and metric designs 
            that better capture the visual-grounded reliability in VLMs for trustworthy 
            real-world autonomous driving.
          </p>
        </div>
      </div>
    </div><hr>
  </div>
</section>




<section class="section" style="margin-top: -10px;">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">
        <span class="icon"><i class="fas fa-bus"></i></span>&thinsp;&thinsp;<span style="color: rgb(66, 133, 244);">Drive</span><span style="color: rgb(192, 0, 0);">Bench</span>: Driving with VLMs
      </h2>
      <div class="pipeline" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
        <img src="static/figures/bench.png" style="width:80%"
            class="pipeline image" alt="benchmark"/>
        <br/>
        <p><strong>Overview of key features and configurations in</strong> <span style="font-family: 'Nunito', sans-serif; color: rgb(66, 133, 244);">Drive</span><span style="font-family: 'Nunito', sans-serif; color: rgb(192, 0, 0);">Bench</span>. Our benchmark encompasses four mainstream driving tasks under <strong>17</strong> settings (clean, corrupted, and text-only visual inputs). The benchmark has <strong>19,200</strong> images and <strong>20,498</strong> QA pairs in total, spanning across <strong>3</strong> question types.</p>
      </div><hr>
    </div>
  </div>
</section>



<section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/teaser.jpg" style="max-width: 100%; height: auto;" />
        </div>
        <!-- Right Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-left: 40px;">
            <h2 class="title is-3">
              <span class="icon"><i class="fa fa-question-circle"></i></span>&thinsp; Are Existing VLMs Ready for Autonomous Driving?
            </h2>
            <p>
              We study this problem from perspectives on reliability, data, and metrics. 
              We find existing VLMs tend to fabricate quality answers to driving questions when visual information is absent. The fabricated answers can bypass current metrics, even GPT scores, due to dataset imbalance, lack of context dataset, and problematical evaluation protocols. 
              Our observations challenge the passive assumption that VLMs are more reliable than task-specific models in driving decisions because of visual-grounded interpretable responses.</p>
          </div>
        </div>
      </div><hr>
    </div>
  </section>

  <br/>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">
        <span class="icon"><i class="fas fa-child"></i></span>&thinsp;Interactive Demo
      </h2>
      <div class="columns">
        <!-- Sidebar (button groups) -->
        <div class="column is-one-quarter">
          <div style="margin-bottom: 1rem;">
            <p class="menu-label">Case</p>
            <div id="caseButtons" style="display: flex; gap: 0.5rem;"></div>
          </div>
  
          <div style="margin-bottom: 1rem;">
            <p class="menu-label">Model</p>
            <div id="modelButtons" style="display: flex; flex-wrap: wrap; gap: 0.5rem;"></div>
          </div>
  
          <div style="margin-bottom: 1rem;">
            <p class="menu-label">Corruption</p>
            <div id="corruptionButtons" style="display: flex; flex-wrap: wrap; gap: 0.5rem;"></div>
          </div>
  
          <div style="margin-bottom: 1rem;">
            <p class="menu-label">Prompt</p>
            <div id="promptButtons" style="display: flex; flex-wrap: wrap; gap: 0.5rem;"></div>
          </div>
        </div>
  
        <!-- Content Area -->
        <div class="column">
          <!-- Images Container -->
          <div id="imageContainer" class="columns is-multiline is-centered"></div>
          <hr>
  
          <!-- Rows for icons and text -->
          <div class="columns">
            <div class="column is-one-quarter">
              <img src="./static/images/icon1.png" alt="Icon 1" style="width: 30%; max-width: 100px; float: right;">
            </div>
            <div class="column" id="questionContainer"
                 style="max-height: 150px; overflow-y: auto; padding: 10px; border: 1px solid #ccc; margin: 10px; box-sizing: border-box;">
              <!-- Question text goes here -->
            </div>
          </div>
  
          <div class="columns">
            <div class="column is-one-quarter">
              <img src="./static/images/icon2.png" alt="Icon 2" style="width: 30%; max-width: 100px; float: right;">
            </div>
            <div class="column" id="answerContainer"
                 style="max-height: 150px; overflow-y: auto; padding: 10px; border: 1px solid #ccc; margin: 10px; box-sizing: border-box;">
              <!-- Model’s answer goes here -->
            </div>
          </div>
  
          <div class="columns">
            <div class="column is-one-quarter">
              <img src="./static/images/icon3.png" alt="Icon 3" style="width: 30%; max-width: 100px; float: right;">
            </div>
            <div class="column" id="gtContainer"
                 style="max-height: 150px; overflow-y: auto; padding: 10px; border: 1px solid #ccc; margin: 10px; box-sizing: border-box;">
              <!-- Ground-truth answer goes here -->
            </div>
          </div>
  
          <div class="columns">
            <div class="column is-one-quarter">
              <img src="./static/images/icon4.png" alt="Icon 4" style="width: 30%; max-width: 100px; float: right;">
            </div>
            <div class="column" id="evaluatorContainer"
                 style="max-height: 150px; overflow-y: auto; padding: 10px; border: 1px solid #ccc; margin: 10px; box-sizing: border-box;">
              <!-- Evaluator text goes here -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



<section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-right: 40px;">
            <h2 class="title is-3">
              Challenging Cases in Existing Dataset
            </h2>
            <p>
              (a): The black sedan is turning left, indicated by the turn lights. 
              (b): The black sedan is turning right. GPT4-o predicts <b>Going Ahead</b> for these two cases. 
            </p>
            <p>
              (c) and (d) are both <b>Turning Right</b>, but GPT4-o fails to locate the objects based on center pixel positions due to the existence of overlapping or occlusion.
            </p>
          </div>
        </div>
        <!-- Right Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/dataset_cases.jpg" style="max-width: 100%; height: auto;" />
        </div>
      </div><hr>
    </div>
  </section>


  <br/>


  <section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/spatial_distribution.png" style="max-width: 100%; height: auto;" />
        </div>
        <!-- Right Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-left: 40px;">
            <h2 class="title is-3">
              Spatial Distribution
            </h2>
            <p>
              We study the spatial distribution of predictions generated by Qwen2-VL (7B), under the <b>text-only prompts</b>. 
              We find that the model can potentially “guess” the MCQ answers without visual information by leveraging plain text cues,
              e.g., camera and coordinate positions mentioned in the questions, resulting in the hallucination issue.
            </p>
          </div>
        </div>
      </div><hr>
    </div>
  </section>


  <br/><br/><br/>




<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">
      <span class="icon"><i class="fa fa-subway"></i></span>&thinsp; Benchmark Study
    </h2>
        <div class="table-container">
          <table class="table is-bordered is-striped is-hoverable">
            <thead>
              <tr>
                <th>Model</th>
                <th>Size</th>
                <th>Type</th>
                <th><img src="static/icons/perception.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Perception</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/perception.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Perception</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/perception.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Perception</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
                <th><img src="static/icons/prediction.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Prediction</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/prediction.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Prediction</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/prediction.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Prediction</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
                <th><img src="static/icons/planning.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Planning</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/planning.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Planning</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/planning.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Planning</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
                <th><img src="static/icons/behavior.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Behavior</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/behavior.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Behavior</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/behavior.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Behavior</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><span style="color: rgb(0, 176, 80);"><b>Human</b></span></td>
                <td>-</td>
                <td>-</td>
                <td><span style="color: rgb(0, 176, 80);">47.67</span></td>
                <td><span style="color: rgb(0, 176, 80);">38.32</span></td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td><span style="color: rgb(0, 176, 80);">69.51</span></td>
                <td><span style="color: rgb(0, 176, 80);">54.09</span></td>
                <td>-</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>GPT-4o</a></td>
                <td>-</td>
                <td>Commercial</td>
                <td>35.37</td>
                <td>35.25</td>
                <td>36.48</td>
                <td>51.30</td>
                <td>49.94</td>
                <td>49.05</td>
                <td>75.75</td>
                <td>75.36</td>
                <td>73.21</td>
                <td>45.40</td>
                <td>44.33</td>
                <td>50.03</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>23.22</td>
                <td>22.95</td>
                <td>22.31</td>
                <td>22.02</td>
                <td>17.54</td>
                <td>14.64</td>
                <td>29.15</td>
                <td>31.51</td>
                <td>32.45</td>
                <td>13.60</td>
                <td>13.62</td>
                <td>14.91</td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>13B</td>
                <td>Open</td>
                <td>23.35</td>
                <td>23.37</td>
                <td>22.37</td>
                <td>36.98</td>
                <td>37.78</td>
                <td>23.98</td>
                <td>34.26</td>
                <td>34.99</td>
                <td>38.85</td>
                <td>32.99</td>
                <td>32.43</td>
                <td>32.79</td>
              </tr>
              <tr>
                <td><a>LLaVA-NeXT</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>24.15</td>
                <td>19.62</td>
                <td>13.86</td>
                <td>35.07</td>
                <td>35.89</td>
                <td>28.36</td>
                <td>45.27</td>
                <td>44.36</td>
                <td>27.58</td>
                <td>48.16</td>
                <td>39.44</td>
                <td>11.92</td>
              </tr>
              <tr>
                <td><a>InternVL2</a></td>
                <td>8B</td>
                <td>Open</td>
                <td>32.36</td>
                <td>32.68</td>
                <td>33.60</td>
                <td>45.52</td>
                <td>37.93</td>
                <td>48.89</td>
                <td>53.27</td>
                <td>55.25</td>
                <td>34.56</td>
                <td>54.58</td>
                <td>40.78</td>
                <td>20.14</td>
              </tr>
              <tr>
                <td><a>Phi-3</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>22.88</td>
                <td>23.93</td>
                <td>28.26</td>
                <td>40.11</td>
                <td>37.27</td>
                <td>22.61</td>
                <td>60.03</td>
                <td>61.31</td>
                <td>46.88</td>
                <td>45.20</td>
                <td>44.57</td>
                <td>28.22</td>
              </tr>
              <tr>
                <td><a>Phi-3.5</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>27.52</td>
                <td>27.51</td>
                <td>28.26</td>
                <td>45.13</td>
                <td>38.21</td>
                <td>4.92</td>
                <td>31.91</td>
                <td>28.36</td>
                <td>46.30</td>
                <td>37.89</td>
                <td>49.13</td>
                <td>39.16</td>
              </tr>
              <tr>
                <td><a>Oryx</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>17.02</td>
                <td>15.97</td>
                <td>18.47</td>
                <td>48.13</td>
                <td>46.63</td>
                <td>12.77</td>
                <td>53.57</td>
                <td>55.76</td>
                <td>48.26</td>
                <td>33.92</td>
                <td>33.81</td>
                <td>23.94</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>28.99</td>
                <td>27.85</td>
                <td>35.16</td>
                <td>37.89</td>
                <td>39.55</td>
                <td>37.77</td>
                <td>57.04</td>
                <td>54.78</td>
                <td>41.66</td>
                <td>49.07</td>
                <td>47.68</td>
                <td>54.48</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>72B</td>
                <td>Open</td>
                <td>30.13</td>
                <td>26.92</td>
                <td>17.70</td>
                <td>49.35</td>
                <td>43.49</td>
                <td>5.57</td>
                <td>61.30</td>
                <td>63.07</td>
                <td>53.35</td>
                <td>51.26</td>
                <td>49.78</td>
                <td>39.46</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>DriveLM</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>16.85</td>
                <td>16.00</td>
                <td>8.75</td>
                <td>44.33</td>
                <td>39.71</td>
                <td>4.70</td>
                <td>68.71</td>
                <td>67.60</td>
                <td>65.24</td>
                <td>42.78</td>
                <td>40.37</td>
                <td>27.83</td>
              </tr>
              <tr>
                <td><a>Dolphins</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>9.59</td>
                <td>10.84</td>
                <td>11.01</td>
                <td>32.66</td>
                <td>29.88</td>
                <td>39.98</td>
                <td>52.91</td>
                <td>53.77</td>
                <td>60.98</td>
                <td>8.81</td>
                <td>8.25</td>
                <td>11.92</td>
              </tr>
            </tbody>
          </table>
          <span>
            <b>Table.</b> <strong>Evaluations of VLMs across different driving tasks</strong> (perception, prediction, planning, and behavior). <span style="color: rgb(0, 176, 80);">Clean</span> represents clean image inputs. <span style="color: rgb(192, 0, 0);">Corr.</span> represents corruption image inputs, averaged across fifteen corruptions. <span style="color: rgb(66, 133, 244);">T.O.</span> represents text-only evaluation. For humans, we only evaluate MCQ questions in perception and behavior tasks. The evaluations are based on GPT scores, where we tailored detailed rubrics for each task and question type.
          </span>
        </div><hr>
  </div>
</div>



<br/><br/>




<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">
      <span class="icon"><i class="fa fa-cogs"></i></span>&thinsp;&thinsp; Robustness Analysis
    </h2>
        <div class="table-container">
          <table class="table is-bordered is-striped is-hoverable">
            <thead>
              <tr>
                <th rowspan="2">Model</th>
                <th rowspan="2">Size</th>
                <th rowspan="2">Type</th>
                <th colspan="3"><img src="static/icons/weather.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/></span>Weather</th>
                <th colspan="3"><img src="static/icons/external.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>External</th>
                <th colspan="3"><img src="static/icons/sensor.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>Sensor</th>
                <th colspan="3"><img src="static/icons/motion.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>Motion</th>
                <th colspan="3"><img src="static/icons/transmission.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>Transmission</th>
              </tr>
              <tr>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>GPT-4o</a></td>
                <td>-</td>
                <td>Commercial</td>
                <td>57.20</td>
                <td>57.28</td>
                <td>54.90</td>
                <td>29.25</td>
                <td>56.60</td>
                <td>61.98</td>
                <td>44.25</td>
                <td>54.95</td>
                <td>56.53</td>
                <td>34.25</td>
                <td>59.20</td>
                <td>56.25</td>
                <td>36.83</td>
                <td>53.95</td>
                <td>57.57</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>69.70</td>
                <td>35.49</td>
                <td>35.91</td>
                <td>26.50</td>
                <td>29.17</td>
                <td>34.95</td>
                <td>18.83</td>
                <td>30.64</td>
                <td>33.15</td>
                <td>71.25</td>
                <td>33.43</td>
                <td>35.18</td>
                <td>10.17</td>
                <td>27.28</td>
                <td>34.38</td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>13B</td>
                <td>Open</td>
                <td>61.60</td>
                <td>39.76</td>
                <td>37.76</td>
                <td>15.50</td>
                <td>34.55</td>
                <td>37.83</td>
                <td>24.08</td>
                <td>35.48</td>
                <td>36.08</td>
                <td>79.75</td>
                <td>36.46</td>
                <td>36.42</td>
                <td>15.50</td>
                <td>32.53</td>
                <td>34.33</td>
              </tr>
              <tr>
                <td><a>LLaVA-NeXT</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>69.70</td>
                <td>36.96</td>
                <td>48.52</td>
                <td>48.50</td>
                <td>30.32</td>
                <td>57.18</td>
                <td>21.83</td>
                <td>30.40</td>
                <td>44.37</td>
                <td>66.00</td>
                <td>34.20</td>
                <td>50.44</td>
                <td>11.83</td>
                <td>29.43</td>
                <td>53.50</td>
              </tr>
              <tr>
                <td><a>InternVL2</a></td>
                <td>8B</td>
                <td>Open</td>
                <td>59.90</td>
                <td>48.72</td>
                <td>48.60</td>
                <td>50.75</td>
                <td>47.74</td>
                <td>57.82</td>
                <td>29.92</td>
                <td>45.06</td>
                <td>51.14</td>
                <td>68.25</td>
                <td>49.51</td>
                <td>49.67</td>
                <td>30.00</td>
                <td>43.42</td>
                <td>54.24</td>
              </tr>
              <tr>
                <td><a>Phi-3</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>40.00</td>
                <td>40.59</td>
                <td>45.61</td>
                <td>25.00</td>
                <td>31.44</td>
                <td>45.99</td>
                <td>16.83</td>
                <td>35.58</td>
                <td>43.71</td>
                <td>31.25</td>
                <td>42.92</td>
                <td>48.43</td>
                <td>27.67</td>
                <td>33.04</td>
                <td>41.35</td>
              </tr>
              <tr>
                <td><a>Phi-3.5</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>60.60</td>
                <td>41.82</td>
                <td>45.97</td>
                <td>21.25</td>
                <td>36.89</td>
                <td>30.95</td>
                <td>25.58</td>
                <td>34.66</td>
                <td>39.30</td>
                <td>33.00</td>
                <td>46.03</td>
                <td>49.33</td>
                <td>39.67</td>
                <td>33.47</td>
                <td>39.67</td>
              </tr>
              <tr>
                <td><a>Oryx</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>53.20</td>
                <td>40.43</td>
                <td>48.95</td>
                <td>45.00</td>
                <td>40.68</td>
                <td>56.06</td>
                <td>50.50</td>
                <td>36.71</td>
                <td>48.55</td>
                <td>72.50</td>
                <td>40.01</td>
                <td>48.33</td>
                <td>39.67</td>
                <td>36.98</td>
                <td>49.87</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>76.70</td>
                <td>49.33</td>
                <td>45.12</td>
                <td>37.50</td>
                <td>47.62</td>
                <td>51.24</td>
                <td>22.83</td>
                <td>39.45</td>
                <td>47.23</td>
                <td>57.00</td>
                <td>47.40</td>
                <td>47.74</td>
                <td>35.83</td>
                <td>42.31</td>
                <td>48.60</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>72B</td>
                <td>Open</td>
                <td>59.80</td>
                <td>51.05</td>
                <td>48.55</td>
                <td>45.50</td>
                <td>50.57</td>
                <td>57.25</td>
                <td>52.25</td>
                <td>45.89</td>
                <td>48.59</td>
                <td>58.25</td>
                <td>50.85</td>
                <td>47.88</td>
                <td>44.83</td>
                <td>46.23</td>
                <td>50.50</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>DriveLM</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>21.20</td>
                <td>42.86</td>
                <td>20.04</td>
                <td>21.25</td>
                <td>37.49</td>
                <td>21.92</td>
                <td>9.00</td>
                <td>36.68</td>
                <td>15.56</td>
                <td>22.25</td>
                <td>42.05</td>
                <td>17.07</td>
                <td>17.50</td>
                <td>39.56</td>
                <td>10.37</td>
              </tr>
              <tr>
                <td><a>Dolphins</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>54.30</td>
                <td>30.21</td>
                <td>31.08</td>
                <td>3.00</td>
                <td>30.42</td>
                <td>29.38</td>
                <td>9.42</td>
                <td>26.83</td>
                <td>26.30</td>
                <td>9.25</td>
                <td>29.82</td>
                <td>28.05</td>
                <td>21.50</td>
                <td>28.86</td>
                <td>27.65</td>
              </tr>
            </tbody>
          </table>
          <span>
            <b>Table.</b> <strong>Evaluations of VLMs across different driving tasks</strong> (perception, prediction, planning, and behavior). <span style="color: rgb(0, 176, 80);">Clean</span> represents clean image inputs. <span style="color: rgb(192, 0, 0);">Corr.</span> represents corruption image inputs, averaged across fifteen corruptions. <span style="color: rgb(66, 133, 244);">T.O.</span> represents text-only evaluation. For humans, we only evaluate MCQ questions in perception and behavior tasks. The evaluations are based on GPT scores, where we tailored detailed rubrics for each task and question type.
          </span>
        </div><hr>
  </div>
</div>




<section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/gpt_rouge_bleu.png" style="max-width: 100%; height: auto;" />
        </div>
        <!-- Right Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-left: 40px;">
            <h2 class="title is-3">
              ROUGE, BLEU, or GPT Score?
            </h2>
            <p>
              Evaluation results when using different metrics. 
              The language metrics, such as <b>ROUGE-L</b> and <b>BLEU-4</b>, exhibit high consistency; 
              while the <b>GPT Score</b> metric demonstrates a noticeable gap compared to existing language metrics. 
              We also observe that fine-tuned process benefits DriveLM significantly in regulating its response format, thus leading to misleading high performance under language metrics.
            </p>
          </div>
        </div>
      </div><hr>
    </div>
  </section>

  <br/>


  <section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-right: 40px;">
            <h2 class="title is-3">
              <span class="icon"><i class="fa fa-industry"></i></span>&thinsp; Behavior Distributions of Steering & Speed in DriveLM
            </h2>
            <p>
              We notice that the majority actions of vehicle behaviors are “Going Ahead”, while only a small proportion of actions are “Turn Left” or “Turn Right”.
            </p>
            <p>
              This leads to the data <b>distribution imbalance</b> issue in evaluating different vision-language models.
            </p>
          </div>
        </div>
        <!-- Right Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/behavior_distribution.png" style="max-width: 100%; height: auto;" />
        </div>
      </div><hr>
    </div>
  </section>


  <br/>


<section class="section" style="margin-top: -50px;"></section>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class=" column is-full-width has-text-centered">
        <h2 class="title is-3">
          <span class="icon"><i class="fa fa-paw"></i></span>&thinsp; GPT4-o Examples
        </h2>
      <img src="static/figures/examples_gpt4o_1.png" />
      <div class="content has-text-justified" style="padding-top: 15px">
        <p>
          <b>Figure.</b> Examples of GPT-4o response to four different tasks and the corresponding evaluation results. 
          GPT-4o is <strong>aware of the dark environment</strong> and can identify the bus, and pedestrians from the images, 
          showing resilience to the dark lighting conditions.
        </p>

        <br/><br/><br/>

      <div class="pipeline" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
        <img src="static/figures/examples_gpt4o_2.png" />

      <div class="content has-text-justified" style="padding-top: 15px">
        <br/>
        <p>
          <b>Figure.</b> Examples of GPT-4o response to four tasks and the corresponding evaluation results under <strong>Motion Blur</strong>. 
          GPT-4o are influenced by the <strong>Motion Blur</strong> and tend to predict <strong>driving fast</strong> based on it. 
          The example shows the potential of visual corruption to influence high-level driving decisions.
        </p>
      </div><hr>

      </div>
      <hr>
    </div>
  </div>
  </div>
  </section>









  <script>
    /**********************************************************
     * Global State
     **********************************************************/
    let selectedCase = 'case1';
    let selectedModel = 'gpt-4o';
    let selectedCorruption = 'clean';
    let selectedPrompt = 'baseline';
  
    let currentCaseData = null;     // Will store Q, A, and image paths
    let currentModelOutput = null;  // Will store the model's answer + all prompts
  
    // For convenience, lists of possible options:
    const cases = ['case1', 'case2'];
    const models = ['gpt-4o', 'qwen-72b', 'drivelm'];
    const corruptions = ['clean', 'text-only', 'snow', 'saturate', 'motion', 'h265'];
    const prompts = ['baseline', 'rubric-aware', 'question-aware', 'context-aware'];
  
    /**********************************************************
     * Utility Functions
     **********************************************************/
    function highlightCoordinates(text, color) {
      if (!text) return '';
      return text
        .replace(/\*\*([^*]+)\*\*/g, `<strong style="color: ${color};">$1</strong>`)
        .replace(/\n/g, '<br>');
    }
  
    // Helper to highlight the currently selected button
    function updateButtonStyles(containerId, selectedValue) {
      const container = document.getElementById(containerId);
      const buttons = container.querySelectorAll('button');
      buttons.forEach(btn => {
        // If button is the selected one
        if (btn.dataset.value === selectedValue) {
          btn.style.backgroundColor = 'rgb(66, 133, 244)';
          btn.style.color = '#fff';
        } else {
          btn.style.backgroundColor = '';
          btn.style.color = '';
        }
      });
    }
  
    // Disable or enable the context-aware prompt button for case2
    function updateContextAwareButtonState() {
      const contextAwareBtn = document.querySelector(
        '#promptButtons button[data-value="context-aware"]'
      );
      if (!contextAwareBtn) return;
  
      if (selectedCase === 'case2') {
        // Disable, gray out
        contextAwareBtn.disabled = true;
        contextAwareBtn.style.backgroundColor = 'gray';
        contextAwareBtn.style.color = '#fff';
      } else {
        // Enable if not case2
        contextAwareBtn.disabled = false;
        // If context-aware is the selected prompt, re-apply highlight
        updateButtonStyles('promptButtons', selectedPrompt);
      }
    }
  
    /**********************************************************
     * Fetching Data
     **********************************************************/
    async function fetchCaseData(caseName) {
      try {
        const response = await fetch(`./static/demos/cases/${caseName}.json`);
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
      } catch (error) {
        console.error('Error fetching case data:', error);
        throw error;
      }
    }
  
    async function fetchModelOutput(caseName, model, corruption) {
      const modelLower = model.toLowerCase();
      const filePath = `./static/demos/res/${caseName}-${modelLower}-${corruption}.json`;
      try {
        const response = await fetch(filePath);
        if (!response.ok) {
          // If the file is missing or there's an error, throw
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        return await response.json();
      } catch (error) {
        console.error('Error fetching model output:', error);
        throw error;
      }
    }
  
    /**********************************************************
     * Update DOM Functions
     **********************************************************/
    // Update only the question and GT from the case data
    function updateQuestionAndGT() {
      const questionContainer = document.getElementById('questionContainer');
      const gtContainer = document.getElementById('gtContainer');
  
      if (!currentCaseData) {
        questionContainer.innerHTML = '<p style="color: red;">No data.</p>';
        gtContainer.innerHTML = '<p style="color: red;">No data.</p>';
        return;
      }
      questionContainer.innerHTML = `
        <p style="background-color: rgb(238,244,250); color: rgb(51,94,150); margin: 0; padding: 10px;">
          <strong>Question:</strong> 
          ${highlightCoordinates(currentCaseData.Q, 'rgb(51,94,150)')}
        </p>
      `;
      gtContainer.innerHTML = `
        <p style="background-color: rgb(250,224,160); color: rgb(224,146,53); margin: 0; padding: 10px;">
          <strong>GT:</strong> 
          ${highlightCoordinates(currentCaseData.A, 'rgb(224,146,53)')}
        </p>
      `;
    }
  
    // Update only the model answer
    function updateModelAnswer() {
      const answerContainer = document.getElementById('answerContainer');
      if (!currentModelOutput) {
        answerContainer.innerHTML = '<p style="color: red;">No model output.</p>';
        return;
      }
      answerContainer.innerHTML = `
        <p style="background-color: rgb(222,241,211); color: rgb(76,124,49); margin: 0; padding: 10px;">
          <strong>Answer:</strong>
          ${highlightCoordinates(currentModelOutput.A, 'rgb(76,124,49)')}
        </p>
      `;
    }
  
    // Update only the evaluator text (depends on the selected prompt)
    function updateEvaluator() {
      const evaluatorContainer = document.getElementById('evaluatorContainer');
      if (!currentModelOutput || !currentModelOutput[selectedPrompt]) {
        evaluatorContainer.innerHTML = '<p style="color: red;">No evaluator output.</p>';
        return;
      }
      evaluatorContainer.innerHTML = `
        <p style="background-color: rgb(236,208,236); color: rgb(110,39,107); margin: 0; padding: 10px;">
          <strong>Evaluator:</strong> 
          ${highlightCoordinates(currentModelOutput[selectedPrompt], 'rgb(110,39,107)')}
        </p>
      `;
    }
  
    // Update images if needed
    function updateImages() {
      const imageContainer = document.getElementById('imageContainer');
      imageContainer.innerHTML = ''; // Clear old images
  
      if (!currentCaseData) return;
  
      let imgPath = currentCaseData.img;
      if (selectedCorruption === 'text-only') {
        imgPath = currentCaseData['text-only-img'];
      } else {
        // If the JSON uses placeholders, replace them
        if (Array.isArray(imgPath)) {
          imgPath = imgPath.map(path => path.replace('{corruption}', selectedCorruption));
        } else {
          imgPath = imgPath.replace('{corruption}', selectedCorruption);
        }
      }
  
      const images = Array.isArray(imgPath) ? imgPath : [imgPath];
      images.forEach((imgSrc, index) => {
        const imageColumn = document.createElement('div');
        imageColumn.className = images.length === 1 ? 'column is-half' : 'column is-one-third';
        imageColumn.innerHTML = `
          <img src="${imgSrc}" alt="Case Image ${index + 1}" style="width: 100%; max-width: 500px;">
        `;
        imageContainer.appendChild(imageColumn);
      });
    }
  
    /**********************************************************
     * Main Update Logic
     **********************************************************/
    // 1) If case changes, fetch new case data + new model data.
    async function handleCaseChange(newCase) {
      selectedCase = newCase;
  
      // If currently set to context-aware and we switch to case2 => revert to baseline
      if (selectedCase === 'case2' && selectedPrompt === 'context-aware') {
        selectedPrompt = 'baseline';
        updateButtonStyles('promptButtons', selectedPrompt);
      }
  
      // Highlight the new selection
      updateButtonStyles('caseButtons', selectedCase);
  
      // Fetch the new case data
      try {
        currentCaseData = await fetchCaseData(selectedCase);
        updateQuestionAndGT();
        updateImages();
      } catch (err) {
        currentCaseData = null;
        console.error(err);
      }
  
      // Then fetch model output
      try {
        currentModelOutput = await fetchModelOutput(selectedCase, selectedModel, selectedCorruption);
        updateModelAnswer();
        updateEvaluator();
      } catch (err) {
        currentModelOutput = null;
        console.error(err);
      }
  
      // Disable/enable context-aware prompt if it's case2
      updateContextAwareButtonState();
    }
  
    // 2) If model or corruption changes, only fetch the new model output.
    async function handleModelOrCorruptionChange(type, newValue) {
      if (type === 'model') {
        selectedModel = newValue;
        updateButtonStyles('modelButtons', selectedModel);
      } else if (type === 'corruption') {
        selectedCorruption = newValue;
        updateButtonStyles('corruptionButtons', selectedCorruption);
  
        // If corruption changed, we also update images
        if (currentCaseData) {
          updateImages();
        }
      }
  
      // Now fetch new model output (only if we have case data)
      if (currentCaseData) {
        try {
          currentModelOutput = await fetchModelOutput(selectedCase, selectedModel, selectedCorruption);
          updateModelAnswer();
          updateEvaluator();
        } catch (err) {
          currentModelOutput = null;
          console.error(err);
        }
      }
    }
  
    // 3) If prompt changes, only update the Evaluator text.
    function handlePromptChange(newPrompt) {
      // If user is on case2 and tries context-aware => revert to baseline
      if (selectedCase === 'case2' && newPrompt === 'context-aware') {
        selectedPrompt = 'baseline';
      } else {
        selectedPrompt = newPrompt;
      }
  
      updateButtonStyles('promptButtons', selectedPrompt);
      updateEvaluator();
  
      // Also refresh the disabled state in case we tried to pick context-aware on case2
      updateContextAwareButtonState();
    }
  
    /**********************************************************
     * Initialization - Create Buttons and Bind Events
     **********************************************************/
    function createButtons(containerId, values, onClickFn) {
      const container = document.getElementById(containerId);
      container.innerHTML = ''; // Clear any existing content
  
      values.forEach(val => {
        const btn = document.createElement('button');
        btn.className = 'button';
        btn.textContent = val;
        btn.dataset.value = val;
        btn.addEventListener('click', () => onClickFn(val));
        container.appendChild(btn);
      });
    }
  
    // Create all button groups
    createButtons('caseButtons', cases, (val) => handleCaseChange(val));
    createButtons('modelButtons', models, (val) => handleModelOrCorruptionChange('model', val));
    createButtons('corruptionButtons', corruptions, (val) => handleModelOrCorruptionChange('corruption', val));
    createButtons('promptButtons', prompts, (val) => handlePromptChange(val));
  
    // Initial highlight
    updateButtonStyles('caseButtons', selectedCase);
    updateButtonStyles('modelButtons', selectedModel);
    updateButtonStyles('corruptionButtons', selectedCorruption);
    updateButtonStyles('promptButtons', selectedPrompt);
  
    // Load initial data and set button states
    handleCaseChange(selectedCase);
  </script>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xie2025drivebench,
  author  = {Xie, Shaoyuan and Kong, Lingdong and Dong, Yuhao and Sima, Chonghao and Zhang, Wenwei and Chen, Qi Alfred and Liu, Ziwei and Pan, Liang},
  title   = {Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives},
  journal = {arXiv preprint arXiv:2501.},
  year    = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
              Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
