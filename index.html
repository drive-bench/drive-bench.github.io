<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives">
  <meta name="keywords" content="Autonomous Driving, Vision-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DriveBench - Project Page</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Nunito" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <!-- <script src="./static/js/index.js"></script> -->
</head>


<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <!-- <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div> -->
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/ldkong1205/Robo3D" target="_blank">
            ICCV 2023 | Robo3D 
          </a>
          <a class="navbar-item" href="https://github.com/ldkong1205/RoboDepth" target="_blank">
            NeurIPS 2023 | RoboDepth
          </a>
          <a class="navbar-item" href="https://robodrive-24.github.io/" target="_blank">
            ICRA 2024 | RoboDrive
          </a>
          <a class="navbar-item" href="https://github.com/Daniel-xsy/RoboBEV" target="_blank">
            TPAMI 2024 | RoboBEV
          </a>
          <a class="navbar-item" href="https://github.com/OpenDriveLab/DriveLM" target="_blank">
            ECCV 2024 | DriveLM
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title">
            Are VLMs Ready for Autonomous Driving?<br/>An Empirical Study from the Reliability, Data, and Metric Perspectives
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://daniel-xsy.github.io/" target="_blank">
                <b>Shaoyuan Xie</b></a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ldkong.com/" target="_blank">
                <b>Lingdong Kong</b></a><sup>2,3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=kMui170AAAAJJ&hl=en" target="_blank">
                <b>Yuhao Dong</b></a><sup>2,4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=dgYJ6esAAAAJJ&hl=en" target="_blank">
                <b>Chonghao Sima</b></a><sup>2,5</sup>
            </span>
            <br/>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=QDXADSEAAAAJJ&hl=en" target="_blank">
                <b>Wenwei Zhang</b></a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ics.uci.edu/~alfchen/" target="_blank">
                <b>Qi Alfred Chen</b></a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://liuziwei7.github.io/" target="_blank">
                <b>Ziwei Liu</b></a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ&hl=en" target="_blank">
                <b>Liang Pan</b></a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>University of California, Irvine&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>2</sup>Shanghai AI Laboratory&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>3</sup>National University of Singapore
            </span>
            <span class="author-block">
              <sup>4</sup>S-Lab, Nanyang Technological University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>5</sup>The University of Hong Kong
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors" style="font-size: 0.5rem;">
            <span class="author-block">
              <sup>&#8225</sup>Project Lead&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <sup>&#167;</sup>Corresponding Author
            </span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(66, 133, 244); color: white; border-color: rgb(66, 133, 244);">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span><b>Paper</b></span>
                </a>
              </span>

              &nbsp;
              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(192, 0, 0); color: white; border-color: rgb(192, 0, 0);">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span><b>arXiv</b></span>
                </a>
              </span>
              &nbsp;
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/drive-bench/toolkit" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(66, 133, 244); color: white; border-color: rgb(66, 133, 244);">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span><b>Toolkit</b></span>
                  </a>
              </span>
              &nbsp;
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/drive-bench/arena" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(192, 0, 0); color: white; border-color: rgb(192, 0, 0);">
                  <span class="icon"><i class="fa fa-database" aria-hidden="true"></i></span>
                  <span><b>Dataset</b></span>
                  </a>
              </span>
              &nbsp;
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-dark"
                   style="background-color: rgb(66, 133, 244); color: white; border-color: rgb(66, 133, 244);">
                  <span class="icon"><i class="fa fa-balance-scale" aria-hidden="true"></i></span>
                  <span><b>Leaderboard</b></span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The rapid advancements in Vision-Language Models (VLMs) have sparked significant 
            interest in their application to autonomous driving, with a focus on achieving 
            interpretable driving decisions through natural language interactions. In this work, 
            we question the fundamental assumption that VLMs inherently provide visual-grounded 
            explanations in driving contexts thus improving driving interpretability. 
            The assumption has been widely acknowledged yet systematically examined.
            Through extensive experiments across 17 conditions (including clean image, 
            corrupted image, and text-only inputs), 12 models, and 5 task domains, 
            we comprehensively investigate the reliability of VLMs' visual-grounded 
            explanations in driving scenarios. Our findings reveal that VLMs frequently 
            fabricate plausible driving responses from general knowledge or subtle text cues, 
            especially in scenarios with degraded or absent visual information.
            However, the phenomenon is long-hiding due to significant problems 
            in terms of dataset imbalances, and unreliable evaluations within 
            current driving-with-language benchmarks, which fail to effectively assess 
            the contextual and safety-oriented requirements of autonomous driving tasks. 
            Instead of pushing for the state-of-the-art model under problematical setups, 
            our works highlight the need to rethink the current dataset and metric designs 
            that better capture the visual-grounded reliability in VLMs for trustworthy 
            real-world autonomous driving.
          </p>
        </div>
      </div>
    </div><hr>
  </div>
</section>




<section class="section" style="margin-top: -10px;">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">
        <span class="icon"><i class="fas fa-bus"></i></span>&thinsp;&thinsp;<span style="color: rgb(66, 133, 244);">Drive</span><span style="color: rgb(192, 0, 0);">Bench</span>: Driving with VLMs
      </h2>
      <div class="pipeline" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
        <img src="static/figures/bench.png" style="width:80%"
            class="pipeline image" alt="benchmark"/>
        <br/>
        <p><strong>Overview of key features and configurations in</strong> <span style="font-family: 'Nunito', sans-serif; color: rgb(66, 133, 244);">Drive</span><span style="font-family: 'Nunito', sans-serif; color: rgb(192, 0, 0);">Bench</span>. Our benchmark encompasses four mainstream driving tasks under <strong>17</strong> settings (clean, corrupted, and text-only visual inputs). The benchmark has <strong>19,200</strong> images and <strong>20,498</strong> QA pairs in total, spanning across <strong>3</strong> question types.</p>
      </div><hr>
    </div>
  </div>
</section>



<section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/teaser.jpg" style="max-width: 100%; height: auto;" />
        </div>
        <!-- Right Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-left: 40px;">
            <h2 class="title is-3">
              <span class="icon"><i class="fa fa-question-circle"></i></span>&thinsp; Are Existing VLMs Ready for Autonomous Driving?
            </h2>
            <p>
              We study this problem from perspectives on reliability, data, and metrics. 
              We find existing VLMs tend to fabricate quality answers to driving questions when visual information is absent. The fabricated answers can bypass current metrics, even GPT scores, due to dataset imbalance, lack of context dataset, and problematical evaluation protocols. 
              Our observations challenge the passive assumption that VLMs are more reliable than task-specific models in driving decisions because of visual-grounded interpretable responses.</p>
          </div>
        </div>
      </div><hr>
    </div>
  </section>

  <br/>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">
      <span class="icon"><i class="fas fa-child"></i></span>&thinsp;Interactive Demo
    </h2>
    <div class="columns">
      <!-- Sidebar -->
      <div class="column is-one-quarter">
        <aside class="menu">
          <p class="menu-label">Case</p>
          <div class="select is-fullwidth">
            <select id="caseSelector">
              <option value="case1">Case 1</option>
              <option value="case2">Case 2</option>
              <!-- <option value="case3">Case 3</option> -->
              <!-- <option value="case4">Case 4</option> -->
            </select>
          </div>

          <p class="menu-label">Model</p>
          <div class="select is-fullwidth">
            <select id="modelSelector">
              <option value="gpt-4o">GPT-4o</option>
              <!-- <option value="phi-3.5">Phi-3.5</option> -->
              <!-- <option value="llava-1.5-13b">LLaVA 1.5 13B</option> -->
              <!-- <option value="internvl">InternVL</option> -->
              <option value="qwen-72b">Qwen 72B</option>
              <!-- <option value="dolphin">Dolphin</option> -->
              <option value="drivelm">DriveLM</option>
            </select>
          </div>

          <p class="menu-label">Corruption</p>
          <div class="select is-fullwidth">
            <select id="corruptionSelector">
              <option value="clean">Clean</option>
              <option value="text-only">Text-only</option>
              <!-- <option value="bright">Bright</option> -->
              <!-- <option value="dark">Dark</option> -->
              <option value="snow">Snow</option>
              <!-- <option value="fog">Fog</option> -->
              <!-- <option value="rain">Rain</option> -->
              <!-- <option value="lens">Lens</option> -->
              <!-- <option value="water">Water</option> -->
              <option value="saturate">Saturate</option>
              <option value="motion">Motion</option>
              <!-- <option value="zoom">Zoom</option> -->
              <!-- <option value="bit">Bit</option> -->
              <!-- <option value="quant">Quant</option> -->
              <option value="h265">H.265</option>
            </select>
          </div>

          <p class="menu-label">Prompt</p>
          <div class="select is-fullwidth">
            <select id="promptSelector">
              <option value="baseline">Baseline</option>
              <option value="rubric-aware">Rubric-aware</option>
              <option value="question-aware">Question-aware</option>
              <option value="context-aware">Context-aware</option>
            </select>
          </div>
        </aside>
      </div>

      <!-- Content Area -->
      <div class="column">
        <div id="contentBox" class="content">
        </div>
      </div>
    </div><hr>
  </div>
</section>



<section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-right: 40px;">
            <h2 class="title is-3">
              Challenging Cases in Existing Dataset
            </h2>
            <p>
              (a): The black sedan is turning left, indicated by the turn lights. 
              (b): The black sedan is turning right. GPT4-o predicts <b>Going Ahead</b> for these two cases. 
            </p>
            <p>
              (c) and (d) are both <b>Turning Right</b>, but GPT4-o fails to locate the objects based on center pixel positions due to the existence of overlapping or occlusion.
            </p>
          </div>
        </div>
        <!-- Right Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/dataset_cases.jpg" style="max-width: 100%; height: auto;" />
        </div>
      </div><hr>
    </div>
  </section>


  <br/>


  <section class="section" style="margin-top: -50px;"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="display: flex; align-items: center;">
        <!-- Left Column for the Image -->
        <div class="column is-half" style="text-align: center;">
          <img src="static/figures/spatial_distribution.png" style="max-width: 100%; height: auto;" />
        </div>
        <!-- Right Column for the Text -->
        <div class="column is-half">
          <div class="content has-text-justified" style="padding-left: 40px;">
            <h2 class="title is-3">
              Spatial Distribution
            </h2>
            <p>
              We provide the spatial distribution of predictions from Qwen2-VL, under the <b>text-only prompts</b>. 
              We find that the model can potentially “guess” the MCQ answers without visual information by leveraging plain text cues,
              e.g., camera and coordinate positions mentioned in the questions.
            </p>
          </div>
        </div>
      </div><hr>
    </div>
  </section>


  <br/><br/><br/>




<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">
      <span class="icon"><i class="fa fa-subway"></i></span>&thinsp; Benchmark Study
    </h2>
        <div class="table-container">
          <table class="table is-bordered is-striped is-hoverable">
            <thead>
              <tr>
                <th>Model</th>
                <th>Size</th>
                <th>Type</th>
                <th><img src="static/icons/perception.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Perception</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/perception.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Perception</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/perception.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Perception</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
                <th><img src="static/icons/prediction.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Prediction</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/prediction.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Prediction</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/prediction.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Prediction</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
                <th><img src="static/icons/planning.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Planning</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/planning.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Planning</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/planning.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Planning</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
                <th><img src="static/icons/behavior.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Behavior</span> (<span style="color: rgb(0, 176, 80);">Clean</span>)</th>
                <th><img src="static/icons/behavior.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Behavior</span> (<span style="color: rgb(192, 0, 0);">Corr.</span>)</th>
                <th><img src="static/icons/behavior.png" style="width: 37px; height: 37px; vertical-align: top;"><span>Behavior</span> (<span style="color: rgb(66, 133, 244);">T.O.</span>)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><span style="color: rgb(0, 176, 80);"><b>Human</b></span></td>
                <td>-</td>
                <td>-</td>
                <td><span style="color: rgb(0, 176, 80);">47.67</span></td>
                <td><span style="color: rgb(0, 176, 80);">38.32</span></td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td>-</td>
                <td><span style="color: rgb(0, 176, 80);">69.51</span></td>
                <td><span style="color: rgb(0, 176, 80);">54.09</span></td>
                <td>-</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>GPT-4o</a></td>
                <td>-</td>
                <td>Commercial</td>
                <td>35.37</td>
                <td>35.25</td>
                <td>36.48</td>
                <td>51.30</td>
                <td>49.94</td>
                <td>49.05</td>
                <td>75.75</td>
                <td>75.36</td>
                <td>73.21</td>
                <td>45.40</td>
                <td>44.33</td>
                <td>50.03</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>23.22</td>
                <td>22.95</td>
                <td>22.31</td>
                <td>22.02</td>
                <td>17.54</td>
                <td>14.64</td>
                <td>29.15</td>
                <td>31.51</td>
                <td>32.45</td>
                <td>13.60</td>
                <td>13.62</td>
                <td>14.91</td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>13B</td>
                <td>Open</td>
                <td>23.35</td>
                <td>23.37</td>
                <td>22.37</td>
                <td>36.98</td>
                <td>37.78</td>
                <td>23.98</td>
                <td>34.26</td>
                <td>34.99</td>
                <td>38.85</td>
                <td>32.99</td>
                <td>32.43</td>
                <td>32.79</td>
              </tr>
              <tr>
                <td><a>LLaVA-NeXT</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>24.15</td>
                <td>19.62</td>
                <td>13.86</td>
                <td>35.07</td>
                <td>35.89</td>
                <td>28.36</td>
                <td>45.27</td>
                <td>44.36</td>
                <td>27.58</td>
                <td>48.16</td>
                <td>39.44</td>
                <td>11.92</td>
              </tr>
              <tr>
                <td><a>InternVL2</a></td>
                <td>8B</td>
                <td>Open</td>
                <td>32.36</td>
                <td>32.68</td>
                <td>33.60</td>
                <td>45.52</td>
                <td>37.93</td>
                <td>48.89</td>
                <td>53.27</td>
                <td>55.25</td>
                <td>34.56</td>
                <td>54.58</td>
                <td>40.78</td>
                <td>20.14</td>
              </tr>
              <tr>
                <td><a>Phi-3</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>22.88</td>
                <td>23.93</td>
                <td>28.26</td>
                <td>40.11</td>
                <td>37.27</td>
                <td>22.61</td>
                <td>60.03</td>
                <td>61.31</td>
                <td>46.88</td>
                <td>45.20</td>
                <td>44.57</td>
                <td>28.22</td>
              </tr>
              <tr>
                <td><a>Phi-3.5</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>27.52</td>
                <td>27.51</td>
                <td>28.26</td>
                <td>45.13</td>
                <td>38.21</td>
                <td>4.92</td>
                <td>31.91</td>
                <td>28.36</td>
                <td>46.30</td>
                <td>37.89</td>
                <td>49.13</td>
                <td>39.16</td>
              </tr>
              <tr>
                <td><a>Oryx</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>17.02</td>
                <td>15.97</td>
                <td>18.47</td>
                <td>48.13</td>
                <td>46.63</td>
                <td>12.77</td>
                <td>53.57</td>
                <td>55.76</td>
                <td>48.26</td>
                <td>33.92</td>
                <td>33.81</td>
                <td>23.94</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>28.99</td>
                <td>27.85</td>
                <td>35.16</td>
                <td>37.89</td>
                <td>39.55</td>
                <td>37.77</td>
                <td>57.04</td>
                <td>54.78</td>
                <td>41.66</td>
                <td>49.07</td>
                <td>47.68</td>
                <td>54.48</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>72B</td>
                <td>Open</td>
                <td>30.13</td>
                <td>26.92</td>
                <td>17.70</td>
                <td>49.35</td>
                <td>43.49</td>
                <td>5.57</td>
                <td>61.30</td>
                <td>63.07</td>
                <td>53.35</td>
                <td>51.26</td>
                <td>49.78</td>
                <td>39.46</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>DriveLM</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>16.85</td>
                <td>16.00</td>
                <td>8.75</td>
                <td>44.33</td>
                <td>39.71</td>
                <td>4.70</td>
                <td>68.71</td>
                <td>67.60</td>
                <td>65.24</td>
                <td>42.78</td>
                <td>40.37</td>
                <td>27.83</td>
              </tr>
              <tr>
                <td><a>Dolphins</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>9.59</td>
                <td>10.84</td>
                <td>11.01</td>
                <td>32.66</td>
                <td>29.88</td>
                <td>39.98</td>
                <td>52.91</td>
                <td>53.77</td>
                <td>60.98</td>
                <td>8.81</td>
                <td>8.25</td>
                <td>11.92</td>
              </tr>
            </tbody>
          </table>
          <span>
            <b>Table.</b> <strong>Evaluations of VLMs across different driving tasks</strong> (perception, prediction, planning, and behavior). <span style="color: rgb(0, 176, 80);">Clean</span> represents clean image inputs. <span style="color: rgb(192, 0, 0);">Corr.</span> represents corruption image inputs, averaged across fifteen corruptions. <span style="color: rgb(66, 133, 244);">T.O.</span> represents text-only evaluation. For humans, we only evaluate MCQ questions in perception and behavior tasks. The evaluations are based on GPT scores, where we tailored detailed rubrics for each task and question type.
          </span>
        </div><hr>
  </div>
</div>



<br/><br/>




<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">
      <span class="icon"><i class="fa fa-cogs"></i></span>&thinsp;&thinsp; Robustness Analysis
    </h2>
        <div class="table-container">
          <table class="table is-bordered is-striped is-hoverable">
            <thead>
              <tr>
                <th rowspan="2">Model</th>
                <th rowspan="2">Size</th>
                <th rowspan="2">Type</th>
                <th colspan="3"><img src="static/icons/weather.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/></span>Weather</th>
                <th colspan="3"><img src="static/icons/external.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>External</th>
                <th colspan="3"><img src="static/icons/sensor.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>Sensor</th>
                <th colspan="3"><img src="static/icons/motion.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>Motion</th>
                <th colspan="3"><img src="static/icons/transmission.png" style="width: 45px; height: 45px; vertical-align: top;"><span><br/>Transmission</th>
              </tr>
              <tr>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
                <th><span style="color: rgb(66, 133, 244);">MCQ</span></th>
                <th><span style="color: rgb(192, 0, 0);">VQA</span></th>
                <th><span style="color: rgb(0, 176, 80);">CAP</span></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>GPT-4o</a></td>
                <td>-</td>
                <td>Commercial</td>
                <td>57.20</td>
                <td>57.28</td>
                <td>54.90</td>
                <td>29.25</td>
                <td>56.60</td>
                <td>61.98</td>
                <td>44.25</td>
                <td>54.95</td>
                <td>56.53</td>
                <td>34.25</td>
                <td>59.20</td>
                <td>56.25</td>
                <td>36.83</td>
                <td>53.95</td>
                <td>57.57</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>69.70</td>
                <td>35.49</td>
                <td>35.91</td>
                <td>26.50</td>
                <td>29.17</td>
                <td>34.95</td>
                <td>18.83</td>
                <td>30.64</td>
                <td>33.15</td>
                <td>71.25</td>
                <td>33.43</td>
                <td>35.18</td>
                <td>10.17</td>
                <td>27.28</td>
                <td>34.38</td>
              </tr>
              <tr>
                <td><a>LLaVA-1.5</a></td>
                <td>13B</td>
                <td>Open</td>
                <td>61.60</td>
                <td>39.76</td>
                <td>37.76</td>
                <td>15.50</td>
                <td>34.55</td>
                <td>37.83</td>
                <td>24.08</td>
                <td>35.48</td>
                <td>36.08</td>
                <td>79.75</td>
                <td>36.46</td>
                <td>36.42</td>
                <td>15.50</td>
                <td>32.53</td>
                <td>34.33</td>
              </tr>
              <tr>
                <td><a>LLaVA-NeXT</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>69.70</td>
                <td>36.96</td>
                <td>48.52</td>
                <td>48.50</td>
                <td>30.32</td>
                <td>57.18</td>
                <td>21.83</td>
                <td>30.40</td>
                <td>44.37</td>
                <td>66.00</td>
                <td>34.20</td>
                <td>50.44</td>
                <td>11.83</td>
                <td>29.43</td>
                <td>53.50</td>
              </tr>
              <tr>
                <td><a>InternVL2</a></td>
                <td>8B</td>
                <td>Open</td>
                <td>59.90</td>
                <td>48.72</td>
                <td>48.60</td>
                <td>50.75</td>
                <td>47.74</td>
                <td>57.82</td>
                <td>29.92</td>
                <td>45.06</td>
                <td>51.14</td>
                <td>68.25</td>
                <td>49.51</td>
                <td>49.67</td>
                <td>30.00</td>
                <td>43.42</td>
                <td>54.24</td>
              </tr>
              <tr>
                <td><a>Phi-3</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>40.00</td>
                <td>40.59</td>
                <td>45.61</td>
                <td>25.00</td>
                <td>31.44</td>
                <td>45.99</td>
                <td>16.83</td>
                <td>35.58</td>
                <td>43.71</td>
                <td>31.25</td>
                <td>42.92</td>
                <td>48.43</td>
                <td>27.67</td>
                <td>33.04</td>
                <td>41.35</td>
              </tr>
              <tr>
                <td><a>Phi-3.5</a></td>
                <td>4.2B</td>
                <td>Open</td>
                <td>60.60</td>
                <td>41.82</td>
                <td>45.97</td>
                <td>21.25</td>
                <td>36.89</td>
                <td>30.95</td>
                <td>25.58</td>
                <td>34.66</td>
                <td>39.30</td>
                <td>33.00</td>
                <td>46.03</td>
                <td>49.33</td>
                <td>39.67</td>
                <td>33.47</td>
                <td>39.67</td>
              </tr>
              <tr>
                <td><a>Oryx</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>53.20</td>
                <td>40.43</td>
                <td>48.95</td>
                <td>45.00</td>
                <td>40.68</td>
                <td>56.06</td>
                <td>50.50</td>
                <td>36.71</td>
                <td>48.55</td>
                <td>72.50</td>
                <td>40.01</td>
                <td>48.33</td>
                <td>39.67</td>
                <td>36.98</td>
                <td>49.87</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>7B</td>
                <td>Open</td>
                <td>76.70</td>
                <td>49.33</td>
                <td>45.12</td>
                <td>37.50</td>
                <td>47.62</td>
                <td>51.24</td>
                <td>22.83</td>
                <td>39.45</td>
                <td>47.23</td>
                <td>57.00</td>
                <td>47.40</td>
                <td>47.74</td>
                <td>35.83</td>
                <td>42.31</td>
                <td>48.60</td>
              </tr>
              <tr>
                <td><a>Qwen2-VL</a></td>
                <td>72B</td>
                <td>Open</td>
                <td>59.80</td>
                <td>51.05</td>
                <td>48.55</td>
                <td>45.50</td>
                <td>50.57</td>
                <td>57.25</td>
                <td>52.25</td>
                <td>45.89</td>
                <td>48.59</td>
                <td>58.25</td>
                <td>50.85</td>
                <td>47.88</td>
                <td>44.83</td>
                <td>46.23</td>
                <td>50.50</td>
              </tr>
              <tr>
                <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
              </tr>
              <tr>
                <td><a>DriveLM</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>21.20</td>
                <td>42.86</td>
                <td>20.04</td>
                <td>21.25</td>
                <td>37.49</td>
                <td>21.92</td>
                <td>9.00</td>
                <td>36.68</td>
                <td>15.56</td>
                <td>22.25</td>
                <td>42.05</td>
                <td>17.07</td>
                <td>17.50</td>
                <td>39.56</td>
                <td>10.37</td>
              </tr>
              <tr>
                <td><a>Dolphins</a></td>
                <td>7B</td>
                <td>Specialist</td>
                <td>54.30</td>
                <td>30.21</td>
                <td>31.08</td>
                <td>3.00</td>
                <td>30.42</td>
                <td>29.38</td>
                <td>9.42</td>
                <td>26.83</td>
                <td>26.30</td>
                <td>9.25</td>
                <td>29.82</td>
                <td>28.05</td>
                <td>21.50</td>
                <td>28.86</td>
                <td>27.65</td>
              </tr>
            </tbody>
          </table>
          <span>
            <b>Table.</b> <strong>Evaluations of VLMs across different driving tasks</strong> (perception, prediction, planning, and behavior). <span style="color: rgb(0, 176, 80);">Clean</span> represents clean image inputs. <span style="color: rgb(192, 0, 0);">Corr.</span> represents corruption image inputs, averaged across fifteen corruptions. <span style="color: rgb(66, 133, 244);">T.O.</span> represents text-only evaluation. For humans, we only evaluate MCQ questions in perception and behavior tasks. The evaluations are based on GPT scores, where we tailored detailed rubrics for each task and question type.
          </span>
        </div><hr>
  </div>
</div>




<section class="section" style="margin-top: -50px;"></section>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class=" column is-full-width has-text-centered">
        <h2 class="title is-3">
          <span class="icon"><i class="fa fa-paw"></i></span>&thinsp; GPT4-o Examples
        </h2>
      <img src="static/figures/examples_gpt4o_1.png" />
      <div class="content has-text-justified" style="padding-top: 15px">
        <p>
          <b>Figure.</b> Overview of the <span style="color: #FF8E26;"><b>See</b></span><span style="color: #01B1A0;"><b>Ground</b></span> framework. 
          We first use a 2D-VLM to interpret the query,
          identifying both the target object (e.g., "laptop") and a context-providing anchor (e.g., "chair with floral
          pattern"). A dynamic viewpoint is then selected based on the anchor’s position, enabling the capture of a 2D
          rendered image that aligns with the query’s spatial requirements. Using the Object Lookup Table (OLT), we
          retrieve the 3D bounding boxes of relevant objects, project them onto the 2D image, and apply visual prompts
          to mark visible objects, filtering out occlusions. The image with prompts, along with the spatial
          descriptions and query, are then input into the 2D-VLM for precise localization of the target object.
          Finally, the 2D-VLM outputs the target object’s ID, and we retrieve its 3D bounding box from the OLT to
          provide the final, accurate 3D position in the scene.
        </p>
      </div>
      <hr>
    </div>
  </div>
  </div>
  </section>



<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Benchmark Examples</h2>

    <div class="pipeline" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
      <img src="static/figures/examples_gpt4o_1.png" style="width:78%"
          class="pipeline image" alt="benchmark"/>
      <br/>
      <p>Example of ...</p>
    </div>

    <br/><br/><br/>

    <div class="pipeline" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
      <img src="static/figures/examples_gpt4o_2.png" style="width:78%"
          class="pipeline image" alt="benchmark"/>
      <br/>
      <p>Example of ...</p>
    </div><hr>
  </div>
</div>



<br/>








<script>
  caseSelector.addEventListener('change', () => {
    const selectedCase = caseSelector.value;
    const contextAwareOption = promptSelector.querySelector('option[value="context-aware"]');

    if (selectedCase === 'case2') {
      contextAwareOption.disabled = true;
      if (promptSelector.value === 'context-aware') {
        promptSelector.value = 'baseline';
      }
    } else {
      contextAwareOption.disabled = false;
    }
  });

  // Trigger change event to set initial state
  caseSelector.dispatchEvent(new Event('change'));
</script>

<script>
  const modelSelector = document.getElementById('modelSelector');
  const corruptionSelector = document.getElementById('corruptionSelector');
  const promptSelector = document.getElementById('promptSelector');
  const caseSelector = document.getElementById('caseSelector');
  const contentBox = document.getElementById('contentBox');
  
  async function fetchCaseData(caseName) {
    try {
      const response = await fetch(`./static/demos/cases/${caseName}.json`);
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      return await response.json();
    } catch (error) {
      console.error('Error fetching case data:', error);
      throw error;
    }
  }

  async function fetchModelOutput(caseName, model, corruption) {
    const modelLower = model.toLowerCase();
    const filePath = `./static/demos/res/${caseName}-${modelLower}-${corruption}.json`;
    try {
      const response = await fetch(filePath);
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      return await response.json();
    } catch (error) {
      console.error('Error fetching model output:', error);
      throw error;
    }
  }

  function highlightCoordinates(text, color) {
    return text
      .replace(/\*\*([^*]+)\*\*/g, `<strong style="color: ${color};">$1</strong>`)
      .replace(/\n/g, '<br>');
  }
  
  async function updateContent() {
    const model = modelSelector.value;
    const corruption = corruptionSelector.value;
    const prompt = promptSelector.value;
    const caseOption = caseSelector.value;
  
    contentBox.innerHTML = '';
  
    try {
      // Fetch data for the selected case
      const caseData = await fetchCaseData(caseOption);
  
      // Fetch model output
      const modelOutput = await fetchModelOutput(caseOption, model, corruption);

      // Determine the image path based on the corruption choice
      let imgPath = caseData.img;
      if (corruption === 'text-only') {
        imgPath = caseData['text-only-img'];
      } else {
        if (Array.isArray(imgPath)) {
          imgPath = imgPath.map(path => path.replace('{corruption}', corruption));
        } else {
          imgPath = imgPath.replace('{corruption}', corruption);
        }
      }

      // Add the image content from the JSON file in a 2x3 layout if there are 6 images
      const images = Array.isArray(imgPath) ? imgPath : [imgPath];
      const imageContainer = document.createElement('div');
      imageContainer.className = 'columns is-multiline is-centered';
  
      images.forEach((imgSrc, index) => {
        const imageColumn = document.createElement('div');
        imageColumn.className = images.length === 1 ? 'column is-half' : 'column is-one-third';
        imageColumn.innerHTML = `
          <img src="${imgSrc}" alt="Case Image ${index + 1}" style="width: 100%; max-width: 500px;">
        `;
        imageContainer.appendChild(imageColumn);
      });
  
      contentBox.appendChild(imageContainer);
  
      // Add static icons for the rows (icon1, icon2, icon3, icon4)
      for (let i = 1; i <= 4; i++) {
        const row = document.createElement('div');
        row.className = 'columns';
  
        const iconColumn = document.createElement('div');
        iconColumn.className = 'column is-one-quarter';
        iconColumn.innerHTML = `<img src="./static/images/icon${i}.png" alt="Icon ${i}" style="width: 30%; max-width: 100px; float: right;">`;
  
        const textColumn = document.createElement('div');
        textColumn.className = 'column';
        textColumn.style = 'max-height: 150px; overflow-y: auto; padding: 10px; border: 1px solid #ccc; margin: 10px; box-sizing: border-box;';
        if (i === 1) {
          textColumn.innerHTML = `
            <p style="background-color: rgb(238,244,250); color: rgb(51,94,150); margin: 0; padding: 10px;"><strong>Question:</strong> ${highlightCoordinates(caseData.Q, 'rgb(51,94,150)')}</p>
          `;
        } else if (i === 2) {
          textColumn.innerHTML = `
            <p style="background-color: rgb(222,241,211); color: rgb(76,124,49); margin: 0; padding: 10px;"><strong>Answer:</strong> ${highlightCoordinates(modelOutput.A, 'rgb(76,124,49)')}</p>
          `;
        } else if (i === 3) {
          textColumn.innerHTML = `
            <p style="background-color: rgb(250,224,160); color: rgb(224,146,53); margin: 0; padding: 10px;"><strong>GT:</strong> ${highlightCoordinates(caseData.A, 'rgb(224,146,53)')}</p>
          `;
        } else {
          textColumn.innerHTML = `
            <p style="background-color: rgb(236,208,236); color: rgb(110,39,107); margin: 0; padding: 10px;"><strong>Evaluator:</strong> ${highlightCoordinates(modelOutput[prompt], 'rgb(110,39,107)')}</p>
          `;
        }
  
        row.appendChild(iconColumn);
        row.appendChild(textColumn);
        contentBox.appendChild(row);
      }
  
    } catch (error) {
      contentBox.innerHTML = '<p class="has-text-danger">Failed to load the selected case data.</p>';
    }
  }
  
  modelSelector.addEventListener('change', updateContent);
  corruptionSelector.addEventListener('change', updateContent);
  promptSelector.addEventListener('change', updateContent);
  caseSelector.addEventListener('change', updateContent);
  
  // Initialize content
  updateContent();
</script>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xie2025drivebench,
  author  = {Xie, Shaoyuan and Kong, Lingdong and Dong, Yuhao and Sima, Chonghao and Zhang, Wenwei and Chen, Qi Alfred and Liu, Ziwei and Pan, Liang},
  title   = {Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives},
  journal = {arXiv preprint arXiv:2501.},
  year    = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
              Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
